%%
%% This is file `sample-xelatex.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-xelatex.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2020}
\acmYear{2020}
\acmDOI{}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[]{}{}{}
\acmBooktitle{}
\acmPrice{}
\acmISBN{}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Interpreting Neural Net Responses}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{P. Chandrikasingh}
\email{puja.chandrikasingh@student.uva.nl}
\affiliation{%
  \institution{University of Amsterdam}
  \city{11059842}
}


\author{R. Leushuis}
\email{radmir.leushuis@student.uva.nl}
\affiliation{%
  \institution{University of Amsterdam}
  \city{10988270}}

\author{P. Lintl}
\email{philipp.lintl@student.uva.nl}
\affiliation{%
  \institution{University of Amsterdam}
  \city{12152498}}

\author{A. Vincol}
\email{anca.vincol@student.uva.nl}
\affiliation{%
  \institution{University of Amsterdam}
  \city{12408913}}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Chandrikasingh, Leushuis, Lintl and Vicol}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  A clear and well-documented \LaTeX\ document is presented as an
  article formatted for publication by ACM in a conference proceedings
  or journal publication. Based on the ``acmart'' document class, this
  article presents and explains many of the common variations, as well
  as many of the formatting elements an author may use in the
  preparation of the documentation of their work.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010178.10010224.10010240.10010241</concept_id>
<concept_desc>Computing methodologies~Image representations</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Image representations}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{datasets, neural networks, gaze detection, text tagging NOG DOEN!!!}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
\begin{teaserfigure}
  \includegraphics[width=\textwidth]{sampleteaser}
  \caption{Seattle Mariners at Spring Training, 2010.}
  \Description{Enjoying the baseball game from the third-base
  seats. Ichiro Suzuki preparing to bat.}
  \label{fig:teaser}
\end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
In the last decade, with the rise of computational power, increasingly more advanced and accurate models for solving everyday problems have emerged. This has also lead to the benefits of applying these models in everyday tasks to inflate. Hence, companies are increasingly more often using these models for making influential decisions, which can have a high impact on the lives of people \citep{ribeiro2016should}. Thus, it is crucial to understand the precise dynamics driving these decisions for the purpose of establishing trust in these decisions. The field that concerns itself with this exact topic is transparency analysis. 

The research in this field comprises of understanding the predictions of complex models. This is done in either a global approach, in which the model as a whole is analyzed, or a local approach, in which the individual model predictions are analyzed \citep{ribeiro2016should}. 
%This report focuses on the latter by analyzing one specific method. The analysis is important, as it allows one to inspect the influence of various features on the final prediction and detect possible undesirable effects. The main goal contributes to current literature by studying the reproducibility of the proposed full-gradients method of \citet{srinivas2019full}, which decomposes the neural net response into input and per-neuron sensitivity components.
\citet{srinivas2019full} focuses on the latter by proposing a full-gradients method, which decomposes the neural net response into input and per-neuron sensitivity components. Moreover, for convolutional nets they introduce a saliency map representation, \textit{FullGrad}, that is based on aggregating the full-gradients. This report continues on \citet{srinivas2019full} research by studying the reproducibility of their proposed FullGrad method.

%In this analysis special attention is given to asses reproducibility of their work by reimplementing the algorithm and extending the experiment. 
%Moreover, this report analyses multiple implementations of saliency algorithms, as there is no single formal definition of saliency.
%The main goal, however, is studying the proposed full-gradients method of \citet{srinivas2019full}, which decomposes the neural net response into input and per-neuron sensitivity components. In this analysis special attention is given to asses reproducibility of their work by reimplementing the algorithm and extending the experiment.

The remainder of this report is organized as follows. Firstly, the full-gradients algorithm is explained in detail. Secondly, the experimental settings for evaluating the algorithm are described. Thirdly, the results are presented. Subsequently, the findings are explained. Next, the findings are connected to other papers in the field. Lastly, the findings are summarized.

\section{Method}
Firstly, an important characteristic of saliency methods is that they are unable to capture the two main properties of interpretability: \textit{completeness} and \textit{weak dependence}. \citet{srinivas2019full} solve this by proposing a method, which is able to capture both the notion of global and local importance. They do so by utilizing full-gradients, which are given by the following pair:
%As saliency map-based interpretability methods are unable capture the two key properties \textit{completeness}, notion of global importance, and \textit{weak dependence}, notion of local importance, \citet{srinivas2019full} propose a new method to interpreted neural net responses, namely full-gradients which are given by the following set:
%The full-gradients as proposed by \citet{srinivas2019full} are given by the following set:
\begin{align}
    G=\left(\nabla_{x} f(\mathbf{x}), f^{b}(\mathbf{x})\right) \in \mathbb{R}^{D+F}
\end{align}
where $f : \mathbb{R}^D \rightarrow \mathbb{R}$ is a neural network function with inputs $\mathbf{x} \in \mathbb{R}^D$ and biases $\mathbf{b}\in \mathbb{R}^F$. Note however that this report uses the shorthanded notation $f(\mathbf{x})$ instead of the explicit notation $f(\mathbf{x}, \mathbf{b})$. Furthermore, $\nabla_xf(\mathbf{x})$ represents the input-gradients and $f^b(\mathbf{b}) = \nabla_b f(\mathbf{x}, \mathbf{b})\odot\mathbf{b}$ the bias-gradients. 

%The per-neuron salient maps can be obtained by visualizing a spatial map $\in \mathbb{R}^{D+F}$ for every convolutional filter. Subsequently, by aggregating these maps, one can obtain the per-layer maps. Aggregating these maps over the layers, in turn, will lead to an approximation of the network-wide saliency map. This leads to the following expression for the network-wide saliency map:
%\begin{align}\label{maineq}
%S_{f}(\mathbf{x})=\psi\left(\nabla_{\mathbf{x}} f(\mathbf{x}) \odot \mathbf{x}\right)+\sum_{l \in L} \sum_{c \in c_{l}} \psi\left(f^{b}(\mathbf{x})_{c}\right)
%\end{align}

% \citet{srinivas2019full} provide a approximate saliency map representation, FullGrad, 
In order to interpreted the full-gradients, \citet{srinivas2019full} introduce a projection of full-gradients for convolutional nets, \textit{FullGrad}, for which this report analyzes the reproducibility.\footnote{The FullGrad method is compared to the Random and gradCAM methods.} The per-neuron salient maps can be obtained by visualizing a spatial map $\in \mathbb{R}^{D}$ for every convolutional filter. Subsequently, if a layer $l$ has $c_l$ channels, then one can obtain the per-layer maps by aggregating these maps for all channels $c$ in layer $l$. Aggregating these maps over the layers $l$, in turn, will lead to an approximation of the network-wide saliency map. This leads to the following expression for the network-wide saliency map, where we have a total of $L$ layers:
\begin{align}\label{maineq}
S_{f}(\mathbf{x})=\psi\left(\nabla_{\mathbf{x}} f(\mathbf{x}) \odot \mathbf{x}\right)+\sum_{l \in L} \sum_{c \in c_{l}} \psi\left(f^{b}(\mathbf{x})_{c}\right)
\end{align}
Note that the gradients cannot be visualized directly, hence \citet{srinivas2019full} introduce a post-processing operator $\psi(\cdot)$ that performs standard post-processing steps that ensure good viewing contrast and the right scaling. This function can differ per task. 

%An important side note to this observation is that the experiments as done by \citet{srinivas2019full} concern FullGrad for which this report analysis the reproducibility.

%IETS MET BOOTSTRAPPEN van eq2???
%This report extends the framework of \citet{srnivs2019full} by proposing an asymptotic extension on equation \eqref{maineq}, which makes the estimated network-wide saliency maps have a smaller standard deviation in finite samples. The extension uses the central limit theorem, which states that for enough input samples, the following holds:
%\begin{align}
 %   S_{f}(\mathbf{x}) \overset{p}{\rightarrow} S_0(\mathbf{x})
%\end{align}
%where $S_0(\mathbf{x})$ is the true value of the saliency map. In finite samples, however, the left hand of this result has a non-zero variance and can be quite off the true value. In order to reduce this variance and have a more consistent estimate, one can use bootstrapping. This leads to the following estimate:
%\begin{align}\label{bootstrap}
 %   S_{b}(\mathbf{x}) = \frac{1}{B} \sum^B_{i=1} S_f(\mathbf{x}_i)
%\end{align}
%where $\mathbf{x}_i$ is a set randomly drawn\footnote{Randomly drawn with replacement using the uniform scheme.} from $\mathbf{x}$ and $B$ is the number of bootstrap cycles. One can show that this leads to a reduction in the variance with order $\frac{1}{\sqrt{B}}$. 

% need to consult Cameron et al. econometrics

\section{Experimental setup}

\subsection{Metrics}
Additionally, in order to evaluate the full performance of the saliency algorithms, this reports also expands the metrics proposed by \citet{srinivas2019full} by using two additional metrics, given by the Kullback–Leibler (KL) divergence \citep{Kullback51klDivergence} and the percentage change in the unnormalized output \citep{kou2018theoretical}. Out of these, the KL divergence measures the difference between two probability distribution and is defined as:
\begin{align}
    D_{\mathrm{KL}}(P_k \| Q)=\sum_{x \in \mathcal{X}} P_k(x) \log \left(\frac{P_k(x)}{Q(x)}\right)
\end{align}]
where $Q$ and $P_K$ denote the discrete probability distributions of the output of the network that follow from the final softmax layer. The difference between the two, however, is that for $Q$ the unaltered image is used as input, while for $P_k$ $k\%$ pixels are removed from the input image.
%where $Q$ denotes the output of the network that is the result of the final softmax layer, while $P_k$ denotes the output of the network for an input in which $k\%$ of the pixels have been removed. 
One can use the KL divergence in order to evaluate saliency algorithms in the following manner. If a saliency algorithm is effective in identifying important pixels, removing these will lead to $P_k$ significantly differing from $Q$, which makes $D_{\mathrm{KL}}(P \| Q)$ strictly positive. By ordering the saliency algorithms on the magnitude of $D_{\mathrm{KL}}(P_k \| Q)$ for various levels of $k$ one can evaluate the quality of the different algorithms. Naturally, the converse holds for removing the least salient pixels, as under an ideal saliency algorithm this leads to $P_k \approx Q$ and making $D_{\mathrm{KL}}(P_k \| Q) \approx 0$.

%The second of these metrics uses the percentage change in the output probability as proposed by \citep{kou2018theoretical}. It is defined as the change in the output of the penultimate layer (before the softmax layer) when removing $k\%$ of the pixels and can be interpreted as the change in the raw signal strength of a certain class for a given input. The benefit of this metric over $D_{\mathrm{KL}}(P_k \| Q)$ is that it is invariant under normalization, as it only looks at the change in the signal strength of the most confident class. This makes the metric unique for a certain output in contrast to the KL divergence which can have the same value for various combinations of the output probabilities. Naturally, if a saliency algorithm performs well, this metric will be large when removing the most salient pixels and small when removing the least salient pixels.A notable drawback of the KL divergence, however, is that the metric is sensitive to the normalization of the probabilities by the final softmax layer. It is defined as the change in the output of the penultimate layer (before the softmax layer) when removing $k\%$ of the pixels and can be interpreted as the change in the raw signal strength of a certain class for a given input.The benefit of this metric over $D_{\mathrm{KL}}(P_k \| Q)$ is that it is invariant under normalization, as it only looks at the change in the signal strength of the most confident class. This makes the metric unique for a certain output in contrast to the KL divergence which can have the same value for various combinations of the output probabilities. Naturally, if a saliency algorithm performs well, this metric will be large when removing the most salient pixels and small when removing the least salient pixels.

A drawback of the KL divergence, however, is that it is effected by the normalization of the probabilities (final softmax layer). I.e. removing the $k\%$ most salient pixels does not only reduce the probability of the predicted class, but also increases other probabilities.\footnote{As the output probabilities need to sum up to one.} Moreover, removing the $k\%$ least salient pixels should not effect the probability of the predicted class, but can lead to redistribution of the probabilities of the other classes. This in turn leads to a strictly positive KL divergence, even though the raw signal (activation before the softmax layer) of the predicted class does not change. 

Hence, the last metric utilizes the unnormalized probabilities that are obtained as the output of the penultimate layer (before the softmax layer). This metric uses the percentage change in the unnormalized probability as the result of removing $k\%$ of the pixels. It can be interpreted as the change in the raw signal strength of a certain class for a given input. Removing the $k\%$ most (least) salient pixels, based on a well working saliency algorithm, leads to this metric being high (small) for the predicted class and small (small) for the other classes. 



\subsection{Experiment}
Subsequently, in order to evaluate the quality of saliency maps of the proposed framework, a number of quantitative experiments have to be performed. For this purpose, this subsection provides a discussion on the experimental framework as used by \citet{srinivas2019full}, after which extensions to their set up are proposed.

%In order to evaluate the quality of saliency maps of the proposed framework, a total of two quantitative experiments are performed. Firstly, similar to \citet{srinivas2019full}, an augmented version of the pixel perturbation scheme is used, which instead of analyzing the effect of removing the top $k\%$ most salient pixels, analyzes the effect of removing the top $k\%$ least salient pixels. The main argument for this augmentation is that the change of image pixels to black pixels is likely to lead to high-frequency edges forming in clusters of salient pixels \citep{srinivas2019full}. Hence, \citet{srinivas2019full} argue that the formation of these edges lead to the output varying not due to correct identification of salient pixels but due to these artifact edges creating noise in the image. They continue to state that by instead removing the $k\%$ least salient pixels, this effect is slightly mitigated, making it the preferable metric.
%In order to evaluate the quality of saliency maps of the proposed framework, several quantitative experiments need to be performed. Hence, this subsection firstly provides a discussion on the metrics utilized by \citet{srinivas2019full}, after which alternatives to these metrics are proposed. 

 
%This report expands on both these metrics by relating the change in accuracy under these schemes to the randomly removing pixels.
%provided by metric 1 and 2 by comparing the values with the values if we would remove random pixels instead of the most or least salient ones.
%In addition to these two metrics, the accuracy is considered KAN DIT/IS DIT ZINVOL? and \textbf{metric 3} WAT MEET DIT? WRM ZOUDEN WE DIT DOEN? WAT VERWACHTEN WE VD ACCURACY OF ABS FRAC OUTPUT?Mss zoiets:In addition to these two metrics, the (simple) accuracy is considered, as better model explanation is only useful if the model performs at least as good as the other models.  
%Furthermore, we deepen the understanding provided by metric 1 and 2 by comparing the values with the values if we would remove random pixels instead of the most or least salient ones.  
%METRICS with human predictions --> could use this if we have such a dataset. LINK: %https://ieeexplore.ieee.org/abstract/document/5740916/metrics#metrics
%Vaker geciteerd, ROC curve kunnen we t gebruiken? LINK: %https://ieeexplore.ieee.org/abstract/document/6112774
%Average running time/average precision recall curves LINK: %https://link.springer.com/chapter/10.1007/978-3-642-33712-3_3
The first experimental framework is based on pixel perturbation (PP). It consists of measuring how well a method can identify the unimportant regions of an image. Similar to \citet{srinivas2019full}, this report uses an augmented version of the PP scheme, which consists of analyzing the effect of removing the $k\%$ least salient pixels on the absolute fractional output change. The augmented PP scheme differs from the original scheme, as it does not analyze the effect of removing the $k\%$ most salient pixels. The main argument for this augmentation is the mitigation of high-frequency edges, which are the result of the change of pixels to black pixels \citep{srinivas2019full}. \citet{srinivas2019full} argue that these edges lead to the output varying not due to correct identification of salient pixels, but due to these artifact edges creating noise in the image. They continue to state that by instead removing the $k\%$ least salient pixels, this effect is slightly mitigated, making the augmented PP method a more robust evaluation framework.

The second experimental framework utilized by \citet{srinivas2019full} is based on the Remove And Retrain (ROAR) method. Unlike the PP scheme, it consists of calculating how well a method can identify important regions. The experimental framework, similar to the set up of \citep{hooker2018evaluating}, consists of removing the $k\%$ of the most salient pixels, after which the classifier is retrained on the new data set. Subsequently, using the retrained model, the change in accuracy is analyzed. The benefit of the retraining step in this case is the compensation of the previously mentioned artifact edges.

%and the accuracy is analyzed. The previously mentioned issue of having artifact edges is %compensated by the retraining step, which if omitted leads to possible misleading of the model \citep{srinivas2019full}.
%One can once again use equation \eqref{metric1} in order to arrive at a relative accuracy curve as a function of $k$. In this case an ideal saliency algorithm will lead to a steeply decreasing curve for small values of $k$.

In addition to these two experimental frameworks, this report also uses the original definition of the PP scheme as defined by \citet{wu2004secure}. It consists of analyzing the changes in the output for removing the most salient pixels and is done in order to able to relate the findings of this report to current literature, which contains methods evaluated with only the original PP scheme. Moreover, instead of blacking out pixels in the removal step for both metrics, this report proposes replacing the pixels by the mean value of the image in order to mitigate the creation of artifact edges as mentioned by \citet{srinivas2019full}.\footnote{This is the result of the transition of the remaining pixels to the removed pixel to be smoother in terms of absolute value of the channels \citep{hessel2018quantitative}.} 



%as proposed by \citep{kou2018theoretical}






%Moreover, an extension to the metrics is proposed, which relates the change in the output in the change that is the result of removing pixels randomly. This has the benefit of measuring the performance of an attribute importance scheme relative to a benchmark scheme. One can also see the benefits of such an extension from the results of \citet{srinivas2019full}, as the change in output that is the result using the saliency algorithms does not significantly differ from randomly removing pixels. 

%This leads to the following expression for the metric:
%\begin{align}\label{metric1}
%    P(k) = \frac{1}{n_k} \sum^{n_k}_{i=1} \left( \mathbb{I}[y_i = \hat{y}_i(\bm{x}_i - K(\bm{x}_i, k)) ] \right) - R(\bm{x}_i, k)
%\end{align}where $n_k$ is the number of images of the most confident class\footnote{Defined as the class for which the model achieves the highest test accuracy.}, $\mathbb{I}(a = b)$ is an indicator function, $\hat{y}_i(x)$ is the class prediction of the model for input $\bm{x}_i$, $K(\bm{x}_i, k)$ is a function that alters a input $\bm{x}_i$ by making the $k\%$ least salient pixels black. The benchmark scheme is simply given by $R(\bm{x}_i, k) = \frac{1}{50} \sum^{50}_{j= 1} \tilde{K}(\bm{x}_i, k)$, making it similar to the original scheme with the main difference being the function $\tilde{K}(\bm{x},k)$, which randomly selects $k\%$ of the pixels and makes them black. One can use equation \eqref{metric1} in order to arrive at a relative accuracy curve as a function of $k$ as presented in \citet{srinivas2019full}. Using this curve, various saliency algorithms can be compared where an ideal algorithm leads to a low constant curve with a sudden steep increase for large values of $k$. 


%in order to ensure a 
%- Normale def metric 1
%- Ipv black nemen we average, wat mag omdat stdev groot en mean/median niets betekent 
%- beide veranderen we als in 3 en waarom


%In our report, we propose a slight augmentation to this metric, which does not only analyze how the accuracy changes when $k\%$ of the least salient pixels are excluded, but also relates this change to the accuracy if $k\%$ of the pixels are randomly selected and excluded. This has the benefit of measuring the performance of an attribute importance scheme relative to a benchmark scheme, making it robust to possible cheating such as making every pixel extremely salient. This leads to the following expression for the metric:
%\begin{align}\label{metric1}
 %   P(k) = \frac{1}{n_k} \sum^{n_k}_{i=1} \left( \mathbb{I}[y_i = \hat{y}_i(\bm{x}_i - K(\bm{x}_i, k)) ] \right) - R(\bm{x}_i, k)
%\end{align}
%where $n_k$ is the number of images of the most confident class\footnote{Defined as the class for which the model achieves the highest test accuracy.}, $\mathbb{I}(a = b)$ is an indicator function, $\hat{y}_i(x)$ is the class prediction of the model for input $\bm{x}_i$, $K(\bm{x}_i, k)$ is a function that alters a input $\bm{x}_i$ by making the $k\%$ least salient pixels black. The benchmark scheme is simply given by $R(\bm{x}_i, k) = \frac{1}{50} \sum^{50}_{j= 1} \tilde{K}(\bm{x}_i, k)$, making it similar to the original scheme with the main difference being the function $\tilde{K}(\bm{x},k)$, which randomly selects $k\%$ of the pixels and makes them black. One can use equation \eqref{metric1} in order to arrive at a relative accuracy curve as a function of $k$ as presented in \citet{srinivas2019full}. Using this curve, various saliency algorithms can be compared where an ideal algorithm leads to a low constant curve with a sudden steep increase for large values of $k$. 

%Unlike \citet{srinivas2019full}, we apply the two metrics described above to both the Imagenet 2012 and CIFAR100 dataset in order to get the full picture. 

\subsection{Experiments}
In order to make statements on the comparison of the various saliency algorithms, an experimental set up must be defined. The first stage in performing the experiments, is reproducing the experiments conducted by \citet{srinivas2019full} on the CIFAR100 dataset \citep{cifar}. In order to arrive at a network-wide saliency map, equation \eqref{maineq} is utilized. Due to the limited availability of the full dataset, reproduction of the experiments using Imagenet 2012 are performed on an publicly available subset of it. 

The second stage consists of extending the experiments of \citet{srinivas2019full} in order to determine whether their proposed methodology is indeed better compared to the Random algorithm\footnote{This scheme randomly removes $k\%$ of the pixels as a baseline method to estimate the effect of artifact edge creation, as is done by \cite{srinivas2019full}. However, as a major constraint is the lack of information on the precise implementation, the following implementation has been used. ...how we implemented it...} and the gradCAM algorithm\footnote{This algorithm is also utilized by \cite{srinivas2019full}, who uses it as one of the algorithms to which the FullGrad method is compared. This report uses a similar implementation to \citet{selvaraju2017grad}.} \citep{selvaraju2017grad}. This is done by evaluating all the metrics, mentioned in the previous subsection, for FullGrad, Random and gradCAM on the CIFAR100 dataset. 

%Each metric is calculated with .. runs and MORE IMPORTANT INFO...



% Here w need to put the following:
% \begin{enumerate}
%     \item How many runs we do?
%     \item What data set we use?
%     \item Comparing the curves?
% \end{enumerate}


\section{Results}

Firstly, the absolute fractional change (AFC) in the output of the three saliency algorithms is compared in Figure \ref{fig:metric1}. By comparing the mean AFC, one can see that the gradCam algorithm has the lowest AFC up to $7\%$ of the least salient pixels removed, after which the AFC of the random algorithm drops below it. A possible explanation for this could be that for an increasing number of (least salient) pixels removed, the identification of insalient pixels starts to play less of a role, as this effect is overtaken by the effect of having insufficient information (in the form of pixels) in the image. This effect can also be seen in the findings of \citet{srinivas2019full} in which the AFC of all of the methods come closer together for an increasing $k\%$.

Another important observation from Figure \ref{fig:metric1} is that the FullGrad algorithm underperforms to both the gradCam algorithm up to $7\%$ and to the random algorithm from $7\%$. This finding is in sharp contrast with the results of \citet{srinivas2019full}, who have shown the average AFC of the FullGrad method to be considerably lower compared to the two other methods. However, an important side note to this finding is that the $95\%$ confidence bounds of all of the methods are large, implying insignificant differences between the methods. Moreover, comparing the areas between the confidence bounds to the ones presented by \citet{srinivas2019full}, one can conclude that the mean AFCs fall into both confidence bounds.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.3]{Images/vb_grafiek.jpeg}
    \caption{Metric 1 on CIFAR100}
    \label{fig:metric1}
\end{figure}\\\\\\
$[$Figure with AFC (pixel perturbation) on Imagenet$]$
\\\\
Subsequently, a qualitative analysis is done by comparing saliency maps in Figures \ref{fig:fullgrad1} and \ref{fig:gradcam1}. From these Figures, one can compare the FullGrad algorithm to the gradCAM algorithm. From this comparison follows that one can clearly see that the FullGrad algorithm leads to clearer identification of the object of interest compared to the latter algorithm. This gives some evidence for a slight preference for the FullGrad method over the GradCAM method for this specific image.
\\\\\\
$[$Figure with accuracy (ROAR) on CIFAR100$]$\\\\
$[$Figure with accuracy (ROAR) on Imagenet$]$
\\\\
$[$Figure with KL on CIFAR100$]$\\\\
$[$Figure with KL on Imagenet$]$\\\\

$[$Figure/table with unnormalized probabilities on CIFAR100$]$\\\\
$[$Figure/table with unnormalized probabilities on Imagenet$]$\\\\


\section{Discussion}

The findings found in this report are largely in line with the findings of \citet{srinivas2019full}. A notable difference, however, is the difference in the augmented accuracy curves, which shows that although the results remain stable for either low $k\%$ in PP or high $k\%$ in ROAR, they break down as $k\%$ either increases or decreases respectively. This implies that the proposed saliency algorithm by \citet{srinivas2019full} is effective in detecting the least and most salient pixels, but performs sub-optimally in detecting pixel that do not fit in the two extremes. Future research can focus on expanding the framework in order to be also assign unbiased saliency to these pixels.

The quantitative methods used in this report to evaluate the quality of salient maps focused on removing either the least or the most salient pixels. As this leads to artifact edges appearing in the image, the changes in the accuracy of the model might be the result of these edges and not the removal of the pixels \citep{srinivas2019full}. Even retraining the model as is done in the ROAR scheme will lead to sub-optimal evaluation, since it will lead to the model using sections of the altered input that it had formerly not used \citep{srinivas2019full}. Naturally, this leads to poor performance in explaining the original model. Future work can focus on designing new evaluation frameworks, which are unbiased to the problems described above.

\section{Broader Implications}
One of the first proposed methods for importance attribution of features was proposed by \citet{ribeiro2016should}, who created a framework for approximating the network locally. This enabled one to understand the dynamics of the model in the neighbourhood of a certain input by using these approximations. 

Lundberg paper: creating a tension between
accuracy and interpretability --> FullGrad doesn't reduce accuracy just provides interpretability; SHAP high SHAP is high importance 


Transperency in the dynamics of the model also oallows for detecting bias.

\section{Conclusion}
Conclusion: Summarize your findings and choose an ACM
badge to award the paper.


% \section{Acknowledgments}

% Identification of funding sources and other support, and thanks to
% individuals and groups that assisted in the research and the
% preparation of the work should be included in an acknowledgment
% section, which is placed just before the reference section in your
% document.

% This section has a special environment:
% \begin{verbatim}
%   \begin{acks}
%   ...
%   \end{acks}
% \end{verbatim}
% so that the information contained therein can be more easily collected
% during the article metadata extraction phase, and to ensure
% consistency in the spelling of the section heading.

% Authors should not prepare this section as a numbered or unnumbered {\verb|\section|}; please use the ``{\verb|acks|}'' environment.


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
To Robert, for the bagels and explaining CMYK and color spaces.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

%%
%% If your work has an appendix, this is the place to put it.
\appendix
\newpage
\section{Appendix}
MAG MET SECTION EN SUBSECTION WERKEN... MSS HANDIG

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{Images/fullgrad_resnet18_most_18.png}
    \caption{Saliency map from FullGrad}
    \label{fig:fullgrad1}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{Images/gradcam_resnet18_most_18.png}
    \caption{Saliency map from gradCAM}
    \label{fig:gradcam1}
\end{figure}

\end{document}
\endinput
%%
%% End of file `sample-xelatex.tex'.
